{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wadeaT/Cloud-Computing-Course/blob/main/HW2/index.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "koDNst9aSaQt",
        "outputId": "1ad0641d-a0df-4b44-d223-fe63c155f1c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.12.14)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install requests beautifulsoup4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUpA_QgoSK_x",
        "outputId": "c384f3cf-9c61-4182-c922-1cdcc9164cf7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "#My index\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.stem import PorterStemmer\n",
        "from collections import defaultdict\n",
        "import requests\n",
        "import json\n",
        "import nltk\n",
        "import re\n",
        "nltk.download('punkt')\n",
        "\n",
        "class IndexCreator:\n",
        "    #Class Initialization\n",
        "    def __init__(self, start_url, firebase_url, max_pages=50):\n",
        "        self.start_url = start_url # starting URL to proccess\n",
        "        self.firebase_url = firebase_url.rstrip('/') + '/'  # the firebase URL\n",
        "        self.max_pages = max_pages  # maximum number of pages to process\n",
        "        self.index = defaultdict(dict) #for the index to automatically handle new terms\n",
        "        self.processed_pages = 0 # number of proccessed pages, start at 0\n",
        "        self.visited_urls = set() #visited URLS\n",
        "\n",
        "        # Patterns for URLs we want to follow\n",
        "        self.relevant_patterns = [\n",
        "            # Core Compute Products\n",
        "            r'/products/compute',\n",
        "            r'/compute',  # General compute pages\n",
        "            r'/kubernetes-engine',  # Google Kubernetes Engine (GKE)\n",
        "            r'/container-optimized-os',\n",
        "            r'/run',  # Cloud Run\n",
        "            r'/functions',  # Cloud Functions\n",
        "            r'/app-engine',  # App Engine\n",
        "\n",
        "            # Compute Documentation\n",
        "            r'/compute/docs',  # All compute docs\n",
        "            r'/compute/docs/concepts',\n",
        "            r'/compute/docs/instances',\n",
        "            r'/compute/docs/disks',\n",
        "            r'/compute/docs/networking',\n",
        "            r'/compute/docs/security',\n",
        "            r'/compute/docs/regions-zones',\n",
        "            r'/compute/docs/quickstarts',\n",
        "            r'/compute/docs/tutorials',\n",
        "            r'/compute/docs/containers',\n",
        "\n",
        "            # Container and Kubernetes related\n",
        "            r'/kubernetes-engine/docs',\n",
        "            r'/container-registry/docs',\n",
        "            r'/containers',\n",
        "\n",
        "            # Serverless Computing\n",
        "            r'/serverless',\n",
        "            r'/run/docs',  # Cloud Run documentation\n",
        "            r'/functions/docs',  # Cloud Functions documentation\n",
        "            r'/app-engine/docs',  # App Engine documentation\n",
        "\n",
        "            # Infrastructure and VMs\n",
        "            r'/vpc/docs',  # Virtual Private Cloud\n",
        "            r'/virtual-machines',\n",
        "            r'/vmware-engine',\n",
        "\n",
        "            # Pricing and Planning\n",
        "            r'/compute/pricing',\n",
        "            r'/compute/docs/machine-types',\n",
        "            r'/compute/docs/cpu-platforms',\n",
        "\n",
        "            # Solutions and Reference\n",
        "            r'/solutions/compute',\n",
        "            r'/architecture/compute',\n",
        "            r'/docs/cloud-computing'\n",
        "        ]\n",
        "\n",
        "    #checks if that pattern appears anywhere in the URL string\n",
        "    #Returns True/False for each check\n",
        "    def is_relevant_url(self, url):\n",
        "        \"\"\"Check if URL matches our relevant patterns\"\"\"\n",
        "        return any(pattern in url for pattern in self.relevant_patterns)\n",
        "\n",
        "    def extract_content(self, url):\n",
        "        \"\"\"Extract meaningful content from a page\"\"\"\n",
        "        try:\n",
        "            #makes an HTTP request to get the webpage content\n",
        "            response = requests.get(url)\n",
        "            #Uses BeautifulSoup to parse the HTML into a soup object for easy navigation\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            # Remove navigation, footer, etc.\n",
        "            for elem in soup.find_all(['nav', 'footer', 'script', 'style']):\n",
        "                elem.decompose()\n",
        "\n",
        "            #extracting main content\n",
        "            content_sections = []\n",
        "            # Get main content\n",
        "            main_content = soup.find('main')\n",
        "            if main_content:\n",
        "                content_sections.append(main_content.get_text())\n",
        "\n",
        "            # Get article content\n",
        "            article = soup.find('article')\n",
        "            if article:\n",
        "                content_sections.append(article.get_text())\n",
        "\n",
        "            # Get section content\n",
        "            sections = soup.find_all('section')\n",
        "            for section in sections:\n",
        "                content_sections.append(section.get_text())\n",
        "\n",
        "            return ' '.join(content_sections)\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting content from {url}: {e}\")\n",
        "            return \"\"\n",
        "\n",
        "    def get_links(self, url):\n",
        "        \"\"\"Get relevant links from a page\"\"\"\n",
        "        try:\n",
        "            response = requests.get(url)\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "            links = set() #Using a set to avoid duplicate links\n",
        "\n",
        "            #finds all <a> tags that have an href attribute\n",
        "            for link in soup.find_all('a', href=True):\n",
        "                href = link['href'] #Gets the href value (the URL)\n",
        "                full_url = ''\n",
        "\n",
        "                if href.startswith('http'):\n",
        "                    if 'cloud.google.com' in href:\n",
        "                        full_url = href\n",
        "                elif href.startswith('/'):\n",
        "                    full_url = f\"https://cloud.google.com{href}\"\n",
        "\n",
        "                if full_url and self.is_relevant_url(full_url):\n",
        "                    links.add(full_url)\n",
        "\n",
        "            return list(links)\n",
        "        except Exception as e:\n",
        "            print(f\"Error getting links from {url}: {e}\")\n",
        "            return []\n",
        "\n",
        "    def process_text(self, text):\n",
        "        \"\"\"Process text into stemmed words\"\"\"\n",
        "        stop_words = {'ha','thi','skip','-','&', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
        "                     'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your',\n",
        "                     'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself',\n",
        "                     'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its'}\n",
        "\n",
        "        words = []\n",
        "        for word in text.split():\n",
        "            word = word.lower()\n",
        "            word = ''.join(c for c in word if c.isalnum())\n",
        "            if word and word not in stop_words and len(word) > 2:\n",
        "                words.append(word)\n",
        "        #Uses Porter Stemming algorithm to reduce words to their root form\n",
        "        stemmer = PorterStemmer()\n",
        "        return [stemmer.stem(word) for word in words]\n",
        "\n",
        "    def create_index(self):\n",
        "        \"\"\"Create inverted index from relevant pages\"\"\"\n",
        "        queue = [self.start_url]\n",
        "        doc_id = 1\n",
        "\n",
        "        while queue and self.processed_pages < self.max_pages:\n",
        "            url = queue.pop(0)\n",
        "\n",
        "            if url in self.visited_urls:\n",
        "                continue\n",
        "\n",
        "            print(f\"Processing page {self.processed_pages + 1}/{self.max_pages}: {url}\")\n",
        "\n",
        "            # Extract and process content\n",
        "            content = self.extract_content(url)\n",
        "            if content:\n",
        "                words = self.process_text(content)\n",
        "\n",
        "                # Count word occurrences\n",
        "                word_counts = defaultdict(int)\n",
        "                for word in words:\n",
        "                    word_counts[word] += 1\n",
        "\n",
        "                # Update index\n",
        "                for word, count in word_counts.items():\n",
        "                    self.index[word][str(doc_id)] = {\n",
        "                        \"url\": url,\n",
        "                        \"counter\": count\n",
        "                    }\n",
        "\n",
        "                # Mark as processed\n",
        "                self.visited_urls.add(url)\n",
        "                self.processed_pages += 1\n",
        "                doc_id += 1\n",
        "\n",
        "                # Get new links to process\n",
        "                if self.processed_pages < self.max_pages:\n",
        "                    new_links = self.get_links(url)\n",
        "                    queue.extend([link for link in new_links if link not in self.visited_urls])\n",
        "\n",
        "        print(f\"\\nIndexing complete!\")\n",
        "        print(f\"Pages processed: {self.processed_pages}\")\n",
        "        print(f\"Unique terms indexed: {len(self.index)}\")\n",
        "\n",
        "    def create_final_data(self):\n",
        "        \"\"\"Format index for database storage\"\"\"\n",
        "        return [{\n",
        "            'term': word,\n",
        "            'DocId': docs\n",
        "        } for word, docs in self.index.items()]\n",
        "\n",
        "    def upload_to_firebase(self):\n",
        "        \"\"\"Upload index to Firebase using REST API\"\"\"\n",
        "        print(\"\\nUploading to Firebase...\")\n",
        "\n",
        "        data = self.create_final_data()\n",
        "        index_url = f\"{self.firebase_url}index.json\"\n",
        "\n",
        "        try:\n",
        "            response = requests.put(index_url, json=data)\n",
        "            response.raise_for_status()\n",
        "            print(\"Upload complete! Data stored in Firebase.\")\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error uploading to Firebase: {e}\")\n",
        "\n",
        "    def search_word(self, term):\n",
        "        \"\"\"Search for a word in the index and return matching document IDs\"\"\"\n",
        "        # Stem the search term to match the index\n",
        "        stemmer = PorterStemmer()\n",
        "        stemmed_term = stemmer.stem(term.lower())\n",
        "\n",
        "        # Get matching documents from the index\n",
        "        if stemmed_term in self.index:\n",
        "            return list(self.index[stemmed_term].keys())\n",
        "        return []\n",
        "\n",
        "    def get_term_frequency(self, term, doc_id):\n",
        "        \"\"\"Get the frequency of a term in a specific document\"\"\"\n",
        "        # Stem the term to match the index\n",
        "        stemmer = PorterStemmer()\n",
        "        stemmed_term = stemmer.stem(term.lower())\n",
        "\n",
        "        # Return the term frequency if it exists, otherwise return 0\n",
        "        if stemmed_term in self.index and doc_id in self.index[stemmed_term]:\n",
        "            return self.index[stemmed_term][doc_id][\"counter\"]\n",
        "        return 0\n",
        "\n",
        "    def get_document_url(self, doc_id):\n",
        "        \"\"\"Get the URL for a specific document ID\"\"\"\n",
        "        # Search through the index to find the document URL\n",
        "        for term_data in self.index.values():\n",
        "            if doc_id in term_data:\n",
        "                return term_data[doc_id][\"url\"]\n",
        "        return None\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "99p-LO1yjc2F"
      },
      "outputs": [],
      "source": [
        "def create_and_upload_index(firebase_url):\n",
        "    \"\"\"Creates an index and uploads it to Firebase with the given Firebase URL\"\"\"\n",
        "    # Static starting URL\n",
        "    url = \"https://cloud.google.com/products/compute\"\n",
        "\n",
        "    # Create the IndexCreator object\n",
        "    indexer = IndexCreator(url, firebase_url, max_pages=50)\n",
        "\n",
        "    # Create the index by processing the pages\n",
        "    indexer.create_index()\n",
        "\n",
        "    # Upload the generated index to Firebase\n",
        "    indexer.upload_to_firebase()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Nl4dWVy_aHKx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b651621-432e-4b74-e914-d49d6d3802c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing page 1/50: https://cloud.google.com/products/compute\n",
            "Processing page 2/50: https://cloud.google.com/compute/docs/images/create-custom\n",
            "Processing page 3/50: https://cloud.google.com/compute/docs/containers/deploying-containers\n",
            "Processing page 4/50: https://cloud.google.com/functions/\n",
            "Processing page 5/50: https://cloud.google.com/compute/sole-tenant-pricing\n",
            "Processing page 6/50: https://cloud.google.com/compute/docs/create-linux-vm-instance\n",
            "Processing page 7/50: https://console.cloud.google.com/compute/instances?_gl=1*qsgqme*_ga*MjA3OTc3Nzk3NS4xNzA0ODkxMjM2*_ga_4LYFWVHBEB*MTcwNTU1ODg4OC4yNy4xLjE3MDU1NjM3MjYuMC4wLjA.\n",
            "Processing page 8/50: https://cloud.google.com/compute/docs/memory-optimized-machines#m3_machine_types\n",
            "Processing page 9/50: https://cloud.google.com/compute/docs/machine-resource\n",
            "Processing page 10/50: https://cloud.google.com/vmware-engine?hl=en\n",
            "Processing page 11/50: https://cloud.google.com/compute/docs/general-purpose-machines#n2d_machines\n",
            "Processing page 12/50: https://cloud.google.com/curated-resources/compute-engine\n",
            "Processing page 13/50: https://cloud.google.com/compute/sla\n",
            "Processing page 14/50: https://cloud.google.com/run/docs/deploying\n",
            "Processing page 15/50: https://cloud.google.com/kubernetes-engine/pricing/\n",
            "Processing page 16/50: https://cloud.google.com/solutions/serverless/\n",
            "Processing page 17/50: https://cloud.google.com/compute/docs/images/create-delete-deprecate-private-images\n",
            "Processing page 18/50: https://cloud.google.com/compute/docs/tutorials/building-images\n",
            "Processing page 19/50: https://cloud.google.com/compute/docs/disks#pd_encryption\n",
            "Processing page 20/50: https://cloud.google.com/compute/docs/resource-quotas\n",
            "Processing page 21/50: https://cloud.google.com/compute/docs/general-purpose-machines#c4_series\n",
            "Processing page 22/50: https://cloud.google.com/compute/docs/instances/spot\n",
            "Processing page 23/50: https://cloud.google.com/compute/docs/instances/stop-start-instance\n",
            "Processing page 24/50: https://cloud.google.com/compute/docs/reference/latest/zones/list\n",
            "Processing page 25/50: https://cloud.google.com/compute/docs/sla\n",
            "Processing page 26/50: https://cloud.google.com/compute/docs/quickstart\n",
            "Processing page 27/50: https://cloud.google.com/serverless/\n",
            "Processing page 28/50: https://cloud.google.com/compute/docs/storage-optimized-machines#z3_series\n",
            "Processing page 29/50: https://cloud.google.com/compute/docs/instances/deleting-instance\n",
            "Processing page 30/50: https://cloud.google.com/compute/docs/images#os-compute-support\n",
            "Processing page 31/50: https://cloud.google.com/compute/docs/memory-optimized-machines#x4_machine_types\n",
            "Processing page 32/50: https://cloud.google.com/compute/docs/instances/future-reservations-overview\n",
            "Processing page 33/50: https://cloud.google.com/compute/docs/regions-zones#available\n",
            "Processing page 34/50: https://cloud.google.com/compute/docs/cpu-platforms\n",
            "Processing page 35/50: https://cloud.google.com/compute/docs/sustained-use-discounts?hl=en\n",
            "Processing page 36/50: https://cloud.google.com/compute/docs/vm-manager\n",
            "Processing page 37/50: https://cloud.google.com/compute/docs/disks/performance#type_comparison\n",
            "Processing page 38/50: https://cloud.google.com/compute/disks-image-pricing#localssdpricing\n",
            "Processing page 39/50: https://cloud.google.com/compute/docs/instance-groups/creating-groups-of-managed-instances\n",
            "Processing page 40/50: https://cloud.google.com/compute/all-pricing\n",
            "Processing page 41/50: https://cloud.google.com/compute/docs/regions-zones/viewing-regions-zones\n",
            "Processing page 42/50: https://cloud.google.com/compute/docs/machine-types\n",
            "Processing page 43/50: https://cloud.google.com/migrate/virtual-machines/\n",
            "Processing page 44/50: https://cloud.google.com/migrate/containers/\n",
            "Processing page 45/50: https://cloud.google.com/compute/docs/instances/live-migration-process\n",
            "Processing page 46/50: https://cloud.google.com/kubernetes-engine/docs/tutorials/hello-app\n",
            "Processing page 47/50: https://cloud.google.com/compute/docs/general-purpose-machines#c3_series\n",
            "Processing page 48/50: https://cloud.google.com/compute/docs/release-notes\n",
            "Processing page 49/50: https://cloud.google.com/compute/docs/general-purpose-machines#n4_machine_types\n",
            "Processing page 50/50: https://cloud.google.com/compute/docs/general-purpose-machines#n2_series\n",
            "\n",
            "Indexing complete!\n",
            "Pages processed: 50\n",
            "Unique terms indexed: 5936\n",
            "\n",
            "Uploading to Firebase...\n",
            "Upload complete! Data stored in Firebase.\n",
            "Attempting to fetch from: https://turtlenimbus-3cb66-default-rtdb.firebaseio.com/index.json\n",
            "Successfully retrieved index from database\n",
            "\n",
            "Sample of indexed terms:\n",
            "Term: googl\n",
            "Number of documents: 51\n",
            "Term: cloud\n",
            "Number of documents: 51\n",
            "Term: next\n",
            "Number of documents: 51\n",
            "Term: come\n",
            "Number of documents: 6\n",
            "Term: la\n",
            "Number of documents: 4\n"
          ]
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "import requests\n",
        "from datetime import datetime\n",
        "from nltk.stem import PorterStemmer\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# First, get the index from Firebase\n",
        "def get_index_from_db(firebase_url):\n",
        "    # Ensure the URL is properly formatted\n",
        "    firebase_url = firebase_url.rstrip('/')  # Remove trailing slash if present\n",
        "    index_url = f\"{firebase_url}/index.json\"  # Add /index.json\n",
        "\n",
        "    print(f\"Attempting to fetch from: {index_url}\")  # Debug print\n",
        "\n",
        "    try:\n",
        "        response = requests.get(index_url)\n",
        "        response.raise_for_status()\n",
        "        index_data = response.json()\n",
        "        index = defaultdict(dict)\n",
        "\n",
        "        for entry in index_data:\n",
        "            term = entry['term']\n",
        "            docs = entry['DocId']\n",
        "            index[term] = docs\n",
        "\n",
        "        print(\"Successfully retrieved index from database\")\n",
        "        return index\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error retrieving index from Firebase: {e}\")\n",
        "        return None\n",
        "\n",
        "# Initialize with the correct Firebase URL\n",
        "firebase_url = \"https://turtlenimbus-3cb66-default-rtdb.firebaseio.com\"\n",
        "create_and_upload_index(firebase_url)\n",
        "index = get_index_from_db(firebase_url)\n",
        "\n",
        "# Let's see what terms we have in the index\n",
        "if index:\n",
        "    print(\"\\nSample of indexed terms:\")\n",
        "    sample_terms = list(index.keys())[:5]\n",
        "    for term in sample_terms:\n",
        "        print(f\"Term: {term}\")\n",
        "        print(f\"Number of documents: {len(index[term])}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QkHULSEdrwrD"
      },
      "outputs": [],
      "source": [
        "'''from firebase_admin import credentials, firestore\n",
        "import firebase_admin\n",
        "\n",
        "def initialize_firebase():\n",
        "    \"\"\"Initialize Firebase connection\"\"\"\n",
        "    try:\n",
        "        # Check if already initialized\n",
        "        if not firebase_admin._apps:\n",
        "            cred = credentials.Certificate({\n",
        "                \"type\": \"service_account\",\n",
        "                \"project_id\": \"turtlenimbus-3cb66\",\n",
        "                # Add other credential details from your Firebase service account JSON\n",
        "            })\n",
        "            firebase_admin.initialize_app(cred)\n",
        "        return firestore.client()\n",
        "    except Exception as e:\n",
        "        print(f\"Error initializing Firebase: {e}\")\n",
        "        return None\n",
        "\n",
        "# Initialize Firebase when notebook runs\n",
        "db = initialize_firebase()'''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "JnKRGiNKrwrE"
      },
      "outputs": [],
      "source": [
        "class QueryService:\n",
        "    def __init__(self, index):\n",
        "        self.index = index\n",
        "        self.stemmer = PorterStemmer()\n",
        "        self.queries = {}\n",
        "\n",
        "    def search_word(self, term):\n",
        "        \"\"\"Search for a word in the index and return matching document IDs\"\"\"\n",
        "        stemmed_term = self.stemmer.stem(term.lower())\n",
        "        print(f\"\\nSearching for term: {term}\")\n",
        "        print(f\"Stemmed form: {stemmed_term}\")\n",
        "\n",
        "        if stemmed_term in self.index:\n",
        "            print(f\"Term found in index\")\n",
        "            doc_data = self.index[stemmed_term]\n",
        "\n",
        "            # Handle list structure\n",
        "            if isinstance(doc_data, list):\n",
        "                # Convert list to dictionary format\n",
        "                doc_dict = {}\n",
        "                for i, item in enumerate(doc_data):\n",
        "                    if item is not None:  # Skip None entries\n",
        "                        doc_dict[str(i)] = item\n",
        "                return doc_dict\n",
        "            # Handle dictionary structure\n",
        "            elif isinstance(doc_data, dict):\n",
        "                return doc_data\n",
        "\n",
        "        print(f\"Term not found in index\")\n",
        "        return {}\n",
        "\n",
        "    def get_term_frequency(self, term, doc_id):\n",
        "        \"\"\"Get the frequency of a term in a specific document\"\"\"\n",
        "        stemmed_term = self.stemmer.stem(term.lower())\n",
        "        if stemmed_term in self.index:\n",
        "            doc_data = self.index[stemmed_term]\n",
        "\n",
        "            # Handle list structure\n",
        "            if isinstance(doc_data, list):\n",
        "                try:\n",
        "                    doc_index = int(doc_id)\n",
        "                    if 0 <= doc_index < len(doc_data) and doc_data[doc_index]:\n",
        "                        return doc_data[doc_index].get('counter', 0)\n",
        "                except (ValueError, IndexError):\n",
        "                    pass\n",
        "            # Handle dictionary structure\n",
        "            elif isinstance(doc_data, dict) and doc_id in doc_data:\n",
        "                return doc_data[doc_id].get('counter', 0)\n",
        "        return 0\n",
        "\n",
        "    def get_document_url(self, doc_id, term_data):\n",
        "        \"\"\"Get URL for a document from term data\"\"\"\n",
        "        if isinstance(term_data, list):\n",
        "            try:\n",
        "                doc_index = int(doc_id)\n",
        "                if 0 <= doc_index < len(term_data) and term_data[doc_index]:\n",
        "                    return term_data[doc_index].get('url')\n",
        "            except (ValueError, IndexError):\n",
        "                pass\n",
        "        elif isinstance(term_data, dict) and doc_id in term_data:\n",
        "            return term_data[doc_id].get('url')\n",
        "        return None\n",
        "\n",
        "    def create_query(self, query_terms):\n",
        "        \"\"\"Create and execute a search query\"\"\"\n",
        "        try:\n",
        "            query_id = str(len(self.queries) + 1)\n",
        "            print(f\"\\nProcessing query ID: {query_id}\")\n",
        "            print(f\"Query terms: {query_terms}\")\n",
        "\n",
        "            # Find matching documents for all terms (AND logic)\n",
        "            doc_sets = []\n",
        "            term_data = {}  # Store term data for URL lookup\n",
        "\n",
        "            for term in query_terms:\n",
        "                doc_dict = self.search_word(term)\n",
        "                if doc_dict:\n",
        "                    doc_sets.append(set(doc_dict.keys()))\n",
        "                    stemmed_term = self.stemmer.stem(term.lower())\n",
        "                    term_data[stemmed_term] = self.index[stemmed_term]\n",
        "\n",
        "            # If we have no results for any term, return empty\n",
        "            if not doc_sets:\n",
        "                print(\"No results found for any terms\")\n",
        "                return {\"id\": query_id, \"message\": \"No results found for your query.\"}\n",
        "\n",
        "            # Find intersection of all document sets\n",
        "            results = set.intersection(*doc_sets)\n",
        "\n",
        "            if not results:\n",
        "                print(\"No documents match all terms\")\n",
        "                return {\"id\": query_id, \"message\": \"No results found for your query.\"}\n",
        "\n",
        "            # Rank results\n",
        "            ranked_results = []\n",
        "            print(\"\\nRanking results...\")\n",
        "\n",
        "            for doc_id in results:\n",
        "                # Calculate score as sum of term frequencies\n",
        "                score = sum(self.get_term_frequency(term, doc_id) for term in query_terms)\n",
        "\n",
        "                # Get URL from any term's data\n",
        "                url = None\n",
        "                for term_doc_data in term_data.values():\n",
        "                    url = self.get_document_url(doc_id, term_doc_data)\n",
        "                    if url:\n",
        "                        break\n",
        "\n",
        "                ranked_results.append({\n",
        "                    \"doc_id\": doc_id,\n",
        "                    \"url\": url,\n",
        "                    \"score\": score\n",
        "                })\n",
        "\n",
        "            # Sort by score\n",
        "            ranked_results.sort(key=lambda x: x[\"score\"], reverse=True)\n",
        "            print(f\"Found {len(ranked_results)} matching documents\")\n",
        "\n",
        "            # Store query\n",
        "            self.queries[query_id] = {\n",
        "                'terms': query_terms,\n",
        "                'results': ranked_results,\n",
        "                'timestamp': str(datetime.now())\n",
        "            }\n",
        "\n",
        "            return {\n",
        "                'id': query_id,\n",
        "                'terms': query_terms,\n",
        "                'results': ranked_results,\n",
        "                'timestamp': str(datetime.now())\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during query processing: {str(e)}\")\n",
        "            return {'error': f\"An error occurred while processing the query: {str(e)}\"}\n",
        "\n",
        "# Test code\n",
        "def test_query_service(index):\n",
        "    print(\"\\n=== Testing Query Service ===\")\n",
        "\n",
        "    query_service = QueryService(index)\n",
        "    test_query = [\"cloud\", \"compute\"]\n",
        "\n",
        "    results = query_service.create_query(test_query)\n",
        "\n",
        "    print(\"\\nFinal Results:\")\n",
        "    if 'error' in results:\n",
        "        print(f\"Error: {results['error']}\")\n",
        "    elif 'message' in results:\n",
        "        print(f\"Message: {results['message']}\")\n",
        "    else:\n",
        "        print(f\"Query ID: {results['id']}\")\n",
        "        print(f\"Found {len(results['results'])} results\")\n",
        "        print(\"\\nTop 5 Results:\")\n",
        "        for result in results['results'][:5]:\n",
        "            print(f\"\\nDocument ID: {result['doc_id']}\")\n",
        "            print(f\"URL: {result['url']}\")\n",
        "            print(f\"Score: {result['score']}\")\n",
        "            print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from nltk.stem import PorterStemmer\n",
        "from collections import defaultdict\n",
        "from datetime import datetime\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Get index from Firebase\n",
        "def get_index_from_db(firebase_url):\n",
        "    firebase_url = firebase_url.rstrip('/')\n",
        "    index_url = f\"{firebase_url}/index.json\"\n",
        "\n",
        "    print(f\"Fetching index from: {index_url}\")\n",
        "\n",
        "    try:\n",
        "        response = requests.get(index_url)\n",
        "        response.raise_for_status()\n",
        "        index_data = response.json()\n",
        "        index = defaultdict(dict)\n",
        "\n",
        "        for entry in index_data:\n",
        "            term = entry['term']\n",
        "            docs = entry['DocId']\n",
        "            index[term] = docs\n",
        "\n",
        "        print(f\"Successfully retrieved index with {len(index)} terms\")\n",
        "        return index\n",
        "    except Exception as e:\n",
        "        print(f\"Error retrieving index: {e}\")\n",
        "        return None\n",
        "\n",
        "class QueryService:\n",
        "    def __init__(self, index):\n",
        "        self.index = index\n",
        "        self.stemmer = PorterStemmer()\n",
        "        self.queries = {}\n",
        "\n",
        "    def search_word(self, term):\n",
        "        stemmed_term = self.stemmer.stem(term.lower())\n",
        "        if stemmed_term in self.index:\n",
        "            doc_data = self.index[stemmed_term]\n",
        "\n",
        "            if isinstance(doc_data, list):\n",
        "                doc_dict = {}\n",
        "                for i, item in enumerate(doc_data):\n",
        "                    if item is not None:\n",
        "                        doc_dict[str(i)] = item\n",
        "                return doc_dict\n",
        "            elif isinstance(doc_data, dict):\n",
        "                return doc_data\n",
        "\n",
        "        return {}\n",
        "\n",
        "    def get_term_frequency(self, term, doc_id):\n",
        "        stemmed_term = self.stemmer.stem(term.lower())\n",
        "        if stemmed_term in self.index:\n",
        "            doc_data = self.index[stemmed_term]\n",
        "\n",
        "            if isinstance(doc_data, list):\n",
        "                try:\n",
        "                    doc_index = int(doc_id)\n",
        "                    if 0 <= doc_index < len(doc_data) and doc_data[doc_index]:\n",
        "                        return doc_data[doc_index].get('counter', 0)\n",
        "                except (ValueError, IndexError):\n",
        "                    pass\n",
        "            elif isinstance(doc_data, dict) and doc_id in doc_data:\n",
        "                return doc_data[doc_id].get('counter', 0)\n",
        "        return 0\n",
        "\n",
        "    def get_document_url(self, doc_id, term_data):\n",
        "        if isinstance(term_data, list):\n",
        "            try:\n",
        "                doc_index = int(doc_id)\n",
        "                if 0 <= doc_index < len(term_data) and term_data[doc_index]:\n",
        "                    return term_data[doc_index].get('url')\n",
        "            except (ValueError, IndexError):\n",
        "                pass\n",
        "        elif isinstance(term_data, dict) and doc_id in term_data:\n",
        "            return term_data[doc_id].get('url')\n",
        "        return None\n",
        "\n",
        "    def create_query(self, query_terms):\n",
        "        try:\n",
        "            query_id = str(len(self.queries) + 1)\n",
        "\n",
        "            # Find matching documents\n",
        "            doc_sets = []\n",
        "            term_data = {}\n",
        "\n",
        "            for term in query_terms:\n",
        "                doc_dict = self.search_word(term)\n",
        "                if doc_dict:\n",
        "                    doc_sets.append(set(doc_dict.keys()))\n",
        "                    stemmed_term = self.stemmer.stem(term.lower())\n",
        "                    term_data[stemmed_term] = self.index[stemmed_term]\n",
        "\n",
        "            if not doc_sets:\n",
        "                return {\"id\": query_id, \"message\": \"No results found for your query.\"}\n",
        "\n",
        "            results = set.intersection(*doc_sets)\n",
        "\n",
        "            if not results:\n",
        "                return {\"id\": query_id, \"message\": \"No results found for your query.\"}\n",
        "\n",
        "            # Rank results\n",
        "            ranked_results = []\n",
        "\n",
        "            for doc_id in results:\n",
        "                score = sum(self.get_term_frequency(term, doc_id) for term in query_terms)\n",
        "                url = None\n",
        "                for term_doc_data in term_data.values():\n",
        "                    url = self.get_document_url(doc_id, term_doc_data)\n",
        "                    if url:\n",
        "                        break\n",
        "\n",
        "                ranked_results.append({\n",
        "                    \"doc_id\": doc_id,\n",
        "                    \"url\": url,\n",
        "                    \"score\": score\n",
        "                })\n",
        "\n",
        "            ranked_results.sort(key=lambda x: x[\"score\"], reverse=True)\n",
        "\n",
        "            return {\n",
        "                'id': query_id,\n",
        "                'terms': query_terms,\n",
        "                'results': ranked_results,\n",
        "                'timestamp': str(datetime.now())\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            return {'error': f\"An error occurred while processing the query: {str(e)}\"}\n",
        "\n",
        "# Simple test code\n",
        "firebase_url = \"https://turtlenimbus-d033a-default-rtdb.firebaseio.com/\"\n",
        "index = get_index_from_db(firebase_url)\n",
        "\n",
        "if index:\n",
        "    # Create query service\n",
        "    query_service = QueryService(index)\n",
        "\n",
        "    # Test with a simple query\n",
        "    test_query = [\"cloud\", \"compute\"]\n",
        "    print(f\"\\nTesting query: {test_query}\")\n",
        "\n",
        "    # Get results\n",
        "    results = query_service.create_query(test_query)\n",
        "\n",
        "    # Display results\n",
        "    print(\"\\nResults:\")\n",
        "    if 'error' in results:\n",
        "        print(f\"Error: {results['error']}\")\n",
        "    elif 'message' in results:\n",
        "        print(f\"Message: {results['message']}\")\n",
        "    else:\n",
        "        print(f\"Query ID: {results['id']}\")\n",
        "        print(f\"Found {len(results['results'])} results\")\n",
        "        print(\"\\nTop 5 Results:\")\n",
        "        for result in results['results'][:5]:\n",
        "            print(f\"\\nDocument ID: {result['doc_id']}\")\n",
        "            print(f\"URL: {result['url']}\")\n",
        "            print(f\"Score: {result['score']}\")\n",
        "            print(\"-\" * 50)\n",
        "else:\n",
        "    print(\"Failed to load index\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjElXZrizQvC",
        "outputId": "1669bcf3-b5c3-4973-de2e-727de5abcad5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching index from: https://turtlenimbus-3cb66-default-rtdb.firebaseio.com/index.json\n",
            "Successfully retrieved index with 6056 terms\n",
            "\n",
            "Testing query: ['cloud', 'compute']\n",
            "\n",
            "Results:\n",
            "Query ID: 1\n",
            "Found 45 results\n",
            "\n",
            "Top 5 Results:\n",
            "\n",
            "Document ID: 46\n",
            "URL: https://cloud.google.com/compute/docs/release-notes\n",
            "Score: 1751\n",
            "--------------------------------------------------\n",
            "\n",
            "Document ID: 13\n",
            "URL: https://cloud.google.com/compute/vm-instance-pricing#sustained_use\n",
            "Score: 677\n",
            "--------------------------------------------------\n",
            "\n",
            "Document ID: 18\n",
            "URL: https://cloud.google.com/compute/docs/images/create-custom\n",
            "Score: 434\n",
            "--------------------------------------------------\n",
            "\n",
            "Document ID: 20\n",
            "URL: https://cloud.google.com/compute/all-pricing\n",
            "Score: 425\n",
            "--------------------------------------------------\n",
            "\n",
            "Document ID: 37\n",
            "URL: https://cloud.google.com/compute/docs/instances/stop-start-instance#starting_a_stopped_instance\n",
            "Score: 404\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}