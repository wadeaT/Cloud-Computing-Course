{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wadeaT/Cloud-Computing-Course/blob/main/index.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "koDNst9aSaQt",
        "outputId": "39d36775-5186-4b8a-d628-4e9e8fddda1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.12.14)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install requests beautifulsoup4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUpA_QgoSK_x",
        "outputId": "82bdec1b-d207-4ff3-bfbd-efc164a5cbf0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing page 1/50: https://cloud.google.com/products/compute\n",
            "Processing page 2/50: https://cloud.google.com/compute/docs/regions-zones/viewing-regions-zones\n",
            "Processing page 3/50: https://cloud.google.com/kubernetes-engine/docs/tutorials/hello-app\n",
            "Processing page 4/50: https://cloud.google.com/compute/docs/projects\n",
            "Processing page 5/50: https://cloud.google.com/compute/docs/general-purpose-machines#c3_series\n",
            "Processing page 6/50: https://cloud.google.com/compute/docs/general-purpose-machines#n1_machines\n",
            "Processing page 7/50: https://cloud.google.com/kubernetes-engine/\n",
            "Processing page 8/50: https://cloud.google.com/compute/docs/storage-optimized-machines#z3_series\n",
            "Processing page 9/50: https://cloud.google.com/kubernetes-engine/pricing/\n",
            "Processing page 10/50: https://cloud.google.com/products/compute/\n",
            "Processing page 11/50: https://cloud.google.com/compute/docs/images/create-delete-deprecate-private-images\n",
            "Processing page 12/50: https://cloud.google.com/products/compute#common-uses\n",
            "Processing page 13/50: https://cloud.google.com/compute/docs/general-purpose-machines#n2_series\n",
            "Processing page 14/50: https://cloud.google.com/compute/docs/cpu-platforms\n",
            "Processing page 15/50: https://console.cloud.google.com/freetrial/?redirectPath=/compute\n",
            "Processing page 16/50: https://cloud.google.com/compute/docs/sustained-use-discounts?hl=en\n",
            "Processing page 17/50: https://cloud.google.com/compute/disks-image-pricing#persistentdisk\n",
            "Processing page 18/50: https://cloud.google.com/compute/docs/reference/latest/zones/list\n",
            "Processing page 19/50: https://cloud.google.com/compute/docs/instances/stop-start-instance#starting_a_stopped_instance\n",
            "Processing page 20/50: https://cloud.google.com/migrate/containers/\n",
            "Processing page 21/50: https://cloud.google.com/compute/docs/images/create-custom\n",
            "Processing page 22/50: https://cloud.google.com/serverless/\n",
            "Processing page 23/50: https://cloud.google.com/compute/\n",
            "Processing page 24/50: https://cloud.google.com/compute/docs/tutorials/building-images\n",
            "Processing page 25/50: https://cloud.google.com/vmware-engine?hl=en\n",
            "Processing page 26/50: https://cloud.google.com/compute/docs/networking/network-overview\n",
            "Processing page 27/50: https://cloud.google.com/functions/\n",
            "Processing page 28/50: https://cloud.google.com/compute/docs/accelerator-optimized-machines\n",
            "Processing page 29/50: https://cloud.google.com/curated-resources/compute-engine\n",
            "Processing page 30/50: https://cloud.google.com/compute/docs/images/creating-custom-windows-byol-images?hl=en\n",
            "Processing page 31/50: https://cloud.google.com/compute/docs/regions-zones#available\n",
            "Processing page 32/50: https://cloud.google.com/compute/docs/images#os-compute-support\n",
            "Processing page 33/50: https://cloud.google.com/compute/docs/resource-quotas\n",
            "Processing page 34/50: https://cloud.google.com/compute/docs/memory-optimized-machines#x4_machine_types\n",
            "Processing page 35/50: https://cloud.google.com/compute/docs/instances/spot\n",
            "Processing page 36/50: https://cloud.google.com/compute/docs/general-purpose-machines#c4_machine_types\n",
            "Processing page 37/50: https://cloud.google.com/compute/docs/create-linux-vm-instance\n",
            "Processing page 38/50: https://cloud.google.com/compute/all-pricing/\n",
            "Processing page 39/50: https://cloud.google.com/compute/docs/instances/deleting-instance\n",
            "Processing page 40/50: https://cloud.google.com/compute/docs/vm-manager\n",
            "Processing page 41/50: https://cloud.google.com/compute/docs/autoscaler\n",
            "Processing page 42/50: https://cloud.google.com/compute/docs/disks/performance#type_comparison\n",
            "Processing page 43/50: https://cloud.google.com/compute/sla\n",
            "Processing page 44/50: https://cloud.google.com/compute/vm-instance-pricing#sustained_use\n",
            "Processing page 45/50: https://cloud.google.com/compute/vm-instance-pricing#committed_use\n",
            "Processing page 46/50: https://cloud.google.com/run/\n",
            "Processing page 47/50: https://cloud.google.com/compute/docs/machine-resource\n",
            "Processing page 48/50: https://cloud.google.com/compute/docs/create-windows-server-vm-instance\n",
            "Processing page 49/50: https://cloud.google.com/compute/docs/instances/apply-sizing-recommendations-for-instances#how_sizing_recommendations_work\n",
            "Processing page 50/50: https://cloud.google.com/compute/docs/faq\n",
            "\n",
            "Indexing complete!\n",
            "Pages processed: 50\n",
            "Unique terms indexed: 5883\n",
            "\n",
            "Uploading to Firebase...\n",
            "Upload complete! Data stored in Firebase.\n"
          ]
        }
      ],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "from nltk.stem import PorterStemmer\n",
        "from collections import defaultdict\n",
        "import requests\n",
        "import json\n",
        "import nltk\n",
        "import re\n",
        "nltk.download('punkt')\n",
        "\n",
        "class IndexCreator:\n",
        "    #Class Initialization\n",
        "    def __init__(self, start_url, firebase_url, max_pages=50):\n",
        "        self.start_url = start_url # starting URL to proccess\n",
        "        self.firebase_url = firebase_url.rstrip('/') + '/'  # the firebase URL\n",
        "        self.max_pages = max_pages  # maximum number of pages to process\n",
        "        self.index = defaultdict(dict) #for the index to automatically handle new terms\n",
        "        self.processed_pages = 0 # number of proccessed pages, start at 0\n",
        "        self.visited_urls = set() #visited URLS\n",
        "\n",
        "        # Patterns for URLs we want to follow\n",
        "        self.relevant_patterns = [\n",
        "            # Core Compute Products\n",
        "            r'/products/compute',\n",
        "            r'/compute',  # General compute pages\n",
        "            r'/kubernetes-engine',  # Google Kubernetes Engine (GKE)\n",
        "            r'/container-optimized-os',\n",
        "            r'/run',  # Cloud Run\n",
        "            r'/functions',  # Cloud Functions\n",
        "            r'/app-engine',  # App Engine\n",
        "\n",
        "            # Compute Documentation\n",
        "            r'/compute/docs',  # All compute docs\n",
        "            r'/compute/docs/concepts',\n",
        "            r'/compute/docs/instances',\n",
        "            r'/compute/docs/disks',\n",
        "            r'/compute/docs/networking',\n",
        "            r'/compute/docs/security',\n",
        "            r'/compute/docs/regions-zones',\n",
        "            r'/compute/docs/quickstarts',\n",
        "            r'/compute/docs/tutorials',\n",
        "            r'/compute/docs/containers',\n",
        "\n",
        "            # Container and Kubernetes related\n",
        "            r'/kubernetes-engine/docs',\n",
        "            r'/container-registry/docs',\n",
        "            r'/containers',\n",
        "\n",
        "            # Serverless Computing\n",
        "            r'/serverless',\n",
        "            r'/run/docs',  # Cloud Run documentation\n",
        "            r'/functions/docs',  # Cloud Functions documentation\n",
        "            r'/app-engine/docs',  # App Engine documentation\n",
        "\n",
        "            # Infrastructure and VMs\n",
        "            r'/vpc/docs',  # Virtual Private Cloud\n",
        "            r'/virtual-machines',\n",
        "            r'/vmware-engine',\n",
        "\n",
        "            # Pricing and Planning\n",
        "            r'/compute/pricing',\n",
        "            r'/compute/docs/machine-types',\n",
        "            r'/compute/docs/cpu-platforms',\n",
        "\n",
        "            # Solutions and Reference\n",
        "            r'/solutions/compute',\n",
        "            r'/architecture/compute',\n",
        "            r'/docs/cloud-computing'\n",
        "        ]\n",
        "\n",
        "    #checks if that pattern appears anywhere in the URL string\n",
        "    #Returns True/False for each check\n",
        "    def is_relevant_url(self, url):\n",
        "        \"\"\"Check if URL matches our relevant patterns\"\"\"\n",
        "        return any(pattern in url for pattern in self.relevant_patterns)\n",
        "\n",
        "    def extract_content(self, url):\n",
        "        \"\"\"Extract meaningful content from a page\"\"\"\n",
        "        try:\n",
        "            #makes an HTTP request to get the webpage content\n",
        "            response = requests.get(url)\n",
        "            #Uses BeautifulSoup to parse the HTML into a soup object for easy navigation\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            # Remove navigation, footer, etc.\n",
        "            for elem in soup.find_all(['nav', 'footer', 'script', 'style']):\n",
        "                elem.decompose()\n",
        "\n",
        "            #extracting main content\n",
        "            content_sections = []\n",
        "            # Get main content\n",
        "            main_content = soup.find('main')\n",
        "            if main_content:\n",
        "                content_sections.append(main_content.get_text())\n",
        "\n",
        "            # Get article content\n",
        "            article = soup.find('article')\n",
        "            if article:\n",
        "                content_sections.append(article.get_text())\n",
        "\n",
        "            # Get section content\n",
        "            sections = soup.find_all('section')\n",
        "            for section in sections:\n",
        "                content_sections.append(section.get_text())\n",
        "\n",
        "            return ' '.join(content_sections)\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting content from {url}: {e}\")\n",
        "            return \"\"\n",
        "\n",
        "    def get_links(self, url):\n",
        "        \"\"\"Get relevant links from a page\"\"\"\n",
        "        try:\n",
        "            response = requests.get(url)\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "            links = set() #Using a set to avoid duplicate links\n",
        "\n",
        "            #finds all <a> tags that have an href attribute\n",
        "            for link in soup.find_all('a', href=True):\n",
        "                href = link['href'] #Gets the href value (the URL)\n",
        "                full_url = ''\n",
        "\n",
        "                if href.startswith('http'):\n",
        "                    if 'cloud.google.com' in href:\n",
        "                        full_url = href\n",
        "                elif href.startswith('/'):\n",
        "                    full_url = f\"https://cloud.google.com{href}\"\n",
        "\n",
        "                if full_url and self.is_relevant_url(full_url):\n",
        "                    links.add(full_url)\n",
        "\n",
        "            return list(links)\n",
        "        except Exception as e:\n",
        "            print(f\"Error getting links from {url}: {e}\")\n",
        "            return []\n",
        "\n",
        "    def process_text(self, text):\n",
        "        \"\"\"Process text into stemmed words\"\"\"\n",
        "        stop_words = {'ha','thi','skip','-','&', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
        "                     'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your',\n",
        "                     'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself',\n",
        "                     'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its'}\n",
        "\n",
        "        words = []\n",
        "        for word in text.split():\n",
        "            word = word.lower()\n",
        "            word = ''.join(c for c in word if c.isalnum())\n",
        "            if word and word not in stop_words and len(word) > 2:\n",
        "                words.append(word)\n",
        "        #Uses Porter Stemming algorithm to reduce words to their root form\n",
        "        stemmer = PorterStemmer()\n",
        "        return [stemmer.stem(word) for word in words]\n",
        "\n",
        "    def create_index(self):\n",
        "        \"\"\"Create inverted index from relevant pages\"\"\"\n",
        "        queue = [self.start_url]\n",
        "        doc_id = 1\n",
        "\n",
        "        while queue and self.processed_pages < self.max_pages:\n",
        "            url = queue.pop(0)\n",
        "\n",
        "            if url in self.visited_urls:\n",
        "                continue\n",
        "\n",
        "            print(f\"Processing page {self.processed_pages + 1}/{self.max_pages}: {url}\")\n",
        "\n",
        "            # Extract and process content\n",
        "            content = self.extract_content(url)\n",
        "            if content:\n",
        "                words = self.process_text(content)\n",
        "\n",
        "                # Count word occurrences\n",
        "                word_counts = defaultdict(int)\n",
        "                for word in words:\n",
        "                    word_counts[word] += 1\n",
        "\n",
        "                # Update index\n",
        "                for word, count in word_counts.items():\n",
        "                    self.index[word][str(doc_id)] = {\n",
        "                        \"url\": url,\n",
        "                        \"counter\": count\n",
        "                    }\n",
        "\n",
        "                # Mark as processed\n",
        "                self.visited_urls.add(url)\n",
        "                self.processed_pages += 1\n",
        "                doc_id += 1\n",
        "\n",
        "                # Get new links to process\n",
        "                if self.processed_pages < self.max_pages:\n",
        "                    new_links = self.get_links(url)\n",
        "                    queue.extend([link for link in new_links if link not in self.visited_urls])\n",
        "\n",
        "        print(f\"\\nIndexing complete!\")\n",
        "        print(f\"Pages processed: {self.processed_pages}\")\n",
        "        print(f\"Unique terms indexed: {len(self.index)}\")\n",
        "\n",
        "    def create_final_data(self):\n",
        "        \"\"\"Format index for database storage\"\"\"\n",
        "        return [{\n",
        "            'term': word,\n",
        "            'DocId': docs\n",
        "        } for word, docs in self.index.items()]\n",
        "\n",
        "    def upload_to_firebase(self):\n",
        "        \"\"\"Upload index to Firebase using REST API\"\"\"\n",
        "        print(\"\\nUploading to Firebase...\")\n",
        "\n",
        "        data = self.create_final_data()\n",
        "        index_url = f\"{self.firebase_url}index.json\"\n",
        "\n",
        "        try:\n",
        "            response = requests.put(index_url, json=data)\n",
        "            response.raise_for_status()\n",
        "            print(\"Upload complete! Data stored in Firebase.\")\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error uploading to Firebase: {e}\")\n",
        "\n",
        "    def search_word(self, term):\n",
        "        \"\"\"Search for a word in the index and return matching document IDs\"\"\"\n",
        "        # Stem the search term to match the index\n",
        "        stemmer = PorterStemmer()\n",
        "        stemmed_term = stemmer.stem(term.lower())\n",
        "\n",
        "        # Get matching documents from the index\n",
        "        if stemmed_term in self.index:\n",
        "            return list(self.index[stemmed_term].keys())\n",
        "        return []\n",
        "\n",
        "    def get_term_frequency(self, term, doc_id):\n",
        "        \"\"\"Get the frequency of a term in a specific document\"\"\"\n",
        "        # Stem the term to match the index\n",
        "        stemmer = PorterStemmer()\n",
        "        stemmed_term = stemmer.stem(term.lower())\n",
        "\n",
        "        # Return the term frequency if it exists, otherwise return 0\n",
        "        if stemmed_term in self.index and doc_id in self.index[stemmed_term]:\n",
        "            return self.index[stemmed_term][doc_id][\"counter\"]\n",
        "        return 0\n",
        "\n",
        "    def get_document_url(self, doc_id):\n",
        "        \"\"\"Get the URL for a specific document ID\"\"\"\n",
        "        # Search through the index to find the document URL\n",
        "        for term_data in self.index.values():\n",
        "            if doc_id in term_data:\n",
        "                return term_data[doc_id][\"url\"]\n",
        "        return None\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "99p-LO1yjc2F"
      },
      "outputs": [],
      "source": [
        "def create_and_upload_index(firebase_url):\n",
        "    \"\"\"Creates an index and uploads it to Firebase with the given Firebase URL\"\"\"\n",
        "    # Static starting URL\n",
        "    url = \"https://cloud.google.com/products/compute\"\n",
        "\n",
        "    # Create the IndexCreator object\n",
        "    indexer = IndexCreator(url, firebase_url, max_pages=50)\n",
        "\n",
        "    # Create the index by processing the pages\n",
        "    indexer.create_index()\n",
        "\n",
        "    # Upload the generated index to Firebase\n",
        "    indexer.upload_to_firebase()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nl4dWVy_aHKx"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "import requests\n",
        "from datetime import datetime\n",
        "from nltk.stem import PorterStemmer\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# First, get the index from Firebase\n",
        "def get_index_from_db(firebase_url):\n",
        "    # Ensure the URL is properly formatted\n",
        "    firebase_url = firebase_url.rstrip('/')  # Remove trailing slash if present\n",
        "    index_url = f\"{firebase_url}/index.json\"  # Add /index.json\n",
        "\n",
        "    print(f\"Attempting to fetch from: {index_url}\")  # Debug print\n",
        "\n",
        "    try:\n",
        "        response = requests.get(index_url)\n",
        "        response.raise_for_status()\n",
        "        index_data = response.json()\n",
        "        index = defaultdict(dict)\n",
        "\n",
        "        for entry in index_data:\n",
        "            term = entry['term']\n",
        "            docs = entry['DocId']\n",
        "            index[term] = docs\n",
        "\n",
        "        print(\"Successfully retrieved index from database\")\n",
        "        return index\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error retrieving index from Firebase: {e}\")\n",
        "        return None\n",
        "\n",
        "# Initialize with the correct Firebase URL\n",
        "firebase_url = \"https://turtlenimbus-3cb66-default-rtdb.firebaseio.com\"\n",
        "create_and_upload_index(firebase_url)\n",
        "index = get_index_from_db(firebase_url)\n",
        "\n",
        "# Let's see what terms we have in the index\n",
        "if index:\n",
        "    print(\"\\nSample of indexed terms:\")\n",
        "    sample_terms = list(index.keys())[:5]\n",
        "    for term in sample_terms:\n",
        "        print(f\"Term: {term}\")\n",
        "        print(f\"Number of documents: {len(index[term])}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''from firebase_admin import credentials, firestore\n",
        "import firebase_admin\n",
        "\n",
        "def initialize_firebase():\n",
        "    \"\"\"Initialize Firebase connection\"\"\"\n",
        "    try:\n",
        "        # Check if already initialized\n",
        "        if not firebase_admin._apps:\n",
        "            cred = credentials.Certificate({\n",
        "                \"type\": \"service_account\",\n",
        "                \"project_id\": \"turtlenimbus-3cb66\",\n",
        "                # Add other credential details from your Firebase service account JSON\n",
        "            })\n",
        "            firebase_admin.initialize_app(cred)\n",
        "        return firestore.client()\n",
        "    except Exception as e:\n",
        "        print(f\"Error initializing Firebase: {e}\")\n",
        "        return None\n",
        "\n",
        "# Initialize Firebase when notebook runs\n",
        "db = initialize_firebase()'''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class QueryService:\n",
        "    def __init__(self, index):\n",
        "        self.index = index\n",
        "        self.stemmer = PorterStemmer()\n",
        "        self.queries = {}\n",
        "\n",
        "    def search_word(self, term):\n",
        "        \"\"\"Search for a word in the index and return matching document IDs\"\"\"\n",
        "        stemmed_term = self.stemmer.stem(term.lower())\n",
        "        if stemmed_term in self.index:\n",
        "            return list(self.index[stemmed_term].keys())\n",
        "        return []\n",
        "\n",
        "    def get_term_frequency(self, term, doc_id):\n",
        "        \"\"\"Get the frequency of a term in a specific document\"\"\"\n",
        "        stemmed_term = self.stemmer.stem(term.lower())\n",
        "        if stemmed_term in self.index and doc_id in self.index[stemmed_term]:\n",
        "            return self.index[stemmed_term][doc_id][\"counter\"]\n",
        "        return 0\n",
        "\n",
        "    def create_query(self, query_terms):\n",
        "        \"\"\"Create and execute a search query\"\"\"\n",
        "        try:\n",
        "            query_id = str(len(self.queries) + 1)\n",
        "\n",
        "            # Find matching documents for all terms (AND logic)\n",
        "            results = None\n",
        "            for term in query_terms:\n",
        "                doc_ids = self.search_word(term)\n",
        "\n",
        "                if results is None:\n",
        "                    results = set(doc_ids)\n",
        "                else:\n",
        "                    results &= set(doc_ids)\n",
        "\n",
        "            if not results:\n",
        "                return {\"query_id\": query_id, \"message\": \"No results found for your query.\"}\n",
        "\n",
        "            # Rank results\n",
        "            ranked_results = []\n",
        "            for doc_id in results:\n",
        "                score = sum(self.get_term_frequency(term, doc_id) for term in query_terms)\n",
        "                ranked_results.append({\"doc_id\": doc_id, \"score\": score})\n",
        "\n",
        "            ranked_results.sort(key=lambda x: x[\"score\"], reverse=True)\n",
        "\n",
        "            return {\n",
        "                'id': query_id,\n",
        "                'terms': query_terms,\n",
        "                'results': ranked_results,\n",
        "                'timestamp': str(datetime.now())\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            return {'error': f\"An error occurred while processing the query: {str(e)}\"}\n",
        "\n",
        "# Create query service with our index\n",
        "query_service = QueryService(index)\n",
        "\n",
        "# Let's test with some terms we know exist in the index\n",
        "test_query = [\"cloud\", \"google\"]\n",
        "results = query_service.create_query(test_query)\n",
        "\n",
        "print(\"\\nQuery Results:\")\n",
        "print(f\"Query ID: {results['id']}\")\n",
        "print(f\"Search Terms: {results['terms']}\")\n",
        "print(f\"\\nTop 5 Results:\")\n",
        "for result in results['results'][:5]:\n",
        "    print(f\"Document ID: {result['doc_id']}, Score: {result['score']}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOWJzH2u+SO1G68xsvm6fVI",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
